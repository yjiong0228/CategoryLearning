{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dd2cbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import entropy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# 确保导入你上传的模块\n",
    "from partitions import Partition\n",
    "from system2 import RationalModelSystem2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0978a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 1. 定义混合模型 (Metacognitive Arbitrator)\n",
    "# ==========================================\n",
    "\n",
    "class MetacognitiveArbitrator:\n",
    "    \"\"\"\n",
    "    元认知仲裁器：管理 System 1 (Rule) 和 System 2 (Exemplar/Cluster) 的交互\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 n_dims=4, \n",
    "                 n_cats=2, \n",
    "                 beta_s1=5.0,        # S1 温度\n",
    "                 arb_threshold=2.0,  # 熵阈值\n",
    "                 arb_slope=5.0,      # 仲裁斜率 (固定或拟合)\n",
    "                 hint_alpha=0.5,     # S2 -> S1 提示强度 (固定或拟合)\n",
    "                 s2_params=None):\n",
    "        \n",
    "        # === System 1: Hypotheses Testing ===\n",
    "        self.s1_core = Partition(n_dims, n_cats)\n",
    "        self.n_hypos = self.s1_core.length\n",
    "        self.s1_posterior = np.ones(self.n_hypos) / self.n_hypos # 初始均匀分布\n",
    "        self.beta_s1 = beta_s1\n",
    "        self.s1_centers = self.s1_core.get_centers() \n",
    "\n",
    "        # === System 2: Rational Model (Anderson) ===\n",
    "        if s2_params is None:\n",
    "            s2_params = {'alpha': 1.0, 'sigma': 0.15, 'l0': 0.001}\n",
    "        self.s2_core = RationalModelSystem2(n_dims, **s2_params)\n",
    "\n",
    "        # === Arbitration Parameters ===\n",
    "        self.arb_threshold = arb_threshold\n",
    "        self.arb_slope = arb_slope\n",
    "        self.hint_alpha = hint_alpha\n",
    "        \n",
    "        # === History Logs ===\n",
    "        self.history = {\n",
    "            'w_s1': [], 'entropy': [], \n",
    "            'p_correct': [], 'human_correct': []\n",
    "        }\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def get_s1_entropy(self):\n",
    "        return entropy(self.s1_posterior)\n",
    "\n",
    "    def calculate_arbitration_weight(self):\n",
    "        \"\"\"计算依赖 System 1 的权重 lambda\"\"\"\n",
    "        H = self.get_s1_entropy()\n",
    "        # 熵越低(H < threshold)，S1 越可信，weight 接近 1\n",
    "        weight = self._sigmoid(-self.arb_slope * (H - self.arb_threshold))\n",
    "        return weight, H\n",
    "\n",
    "    def _compute_s2_hint_signal(self):\n",
    "        \"\"\"S2 -> S1: 基于 Cluster 结构提示可能的规则\"\"\"\n",
    "        s2_centers = [c['sum_x']/c['N'] for c in self.s2_core.clusters]\n",
    "        if not s2_centers:\n",
    "            return np.ones(self.n_hypos)\n",
    "\n",
    "        hint_vector = np.zeros(self.n_hypos)\n",
    "        for h in range(self.n_hypos):\n",
    "            # 获取该假设下的理论中心\n",
    "            theory_centers = np.array(list(self.s1_centers[h][1].values()))\n",
    "            \n",
    "            # 计算 S2 中心与该规则理论中心的匹配度\n",
    "            total_dist = 0\n",
    "            for mu in s2_centers:\n",
    "                dists = np.linalg.norm(theory_centers - mu, axis=1)\n",
    "                total_dist += np.min(dists) ** 2\n",
    "            \n",
    "            avg_dist = total_dist / len(s2_centers)\n",
    "            hint_vector[h] = np.exp(-5.0 * avg_dist) # 距离越近，Hint 越强\n",
    "\n",
    "        return hint_vector\n",
    "\n",
    "    def predict(self, stimulus, return_details=False):\n",
    "        # 1. System 1 预测\n",
    "        # 构造 dummy data 以调用 Partition 接口\n",
    "        dummy_data = ([stimulus], [1], [1])\n",
    "        s1_liks = np.zeros((self.s1_core.n_cats, self.n_hypos))\n",
    "        for h in range(self.n_hypos):\n",
    "            # 获取 P(y|x, h)\n",
    "            prob = self.s1_core.calc_likelihood_base(h, dummy_data, beta=self.beta_s1)\n",
    "            s1_liks[:, h] = prob[:, 0]\n",
    "        p_s1 = np.dot(s1_liks, self.s1_posterior) # 贝叶斯平均\n",
    "        \n",
    "        # 2. System 2 预测\n",
    "        p_s2_dict = self.s2_core.get_choice_probs(stimulus)\n",
    "        p_s2 = np.array([p_s2_dict.get(c+1, 0) for c in range(self.s1_core.n_cats)])\n",
    "        \n",
    "        # 3. 仲裁融合\n",
    "        w_s1, entropy_val = self.calculate_arbitration_weight()\n",
    "        p_final = w_s1 * p_s1 + (1 - w_s1) * p_s2\n",
    "        p_final /= np.sum(p_final) # 归一化\n",
    "        \n",
    "        if return_details:\n",
    "            return p_final, {'w_s1': w_s1, 'p_s1': p_s1, 'p_s2': p_s2, 'entropy': entropy_val}\n",
    "        return p_final\n",
    "\n",
    "    def update(self, stimulus, category):\n",
    "        cat_idx = int(category) - 1\n",
    "        \n",
    "        # 1. 更新 S1 后验\n",
    "        dummy_data = ([stimulus], [1], [1])\n",
    "        likelihoods = np.zeros(self.n_hypos)\n",
    "        for h in range(self.n_hypos):\n",
    "            prob = self.s1_core.calc_likelihood_base(h, dummy_data, beta=self.beta_s1)\n",
    "            likelihoods[h] = prob[cat_idx, 0]\n",
    "        \n",
    "        self.s1_posterior *= likelihoods\n",
    "        if np.sum(self.s1_posterior) > 0:\n",
    "            self.s1_posterior /= np.sum(self.s1_posterior)\n",
    "        else:\n",
    "            self.s1_posterior = np.ones(self.n_hypos) / self.n_hypos\n",
    "            \n",
    "        # 2. 更新 S2\n",
    "        self.s2_core.update(stimulus, category)\n",
    "        \n",
    "        # 3. S2 -> S1 提示 (Bottom-up Hint)\n",
    "        # 仅当主要依赖 S2 (w_s1 低) 且 S2 已经积累了一些经验时触发\n",
    "        w_s1, _ = self.calculate_arbitration_weight()\n",
    "        if w_s1 < 0.8 and self.s2_core.total_N > 5:\n",
    "            hint_vector = self._compute_s2_hint_signal()\n",
    "            self.s1_posterior *= (1 + self.hint_alpha * hint_vector)\n",
    "            if np.sum(self.s1_posterior) > 0:\n",
    "                self.s1_posterior /= np.sum(self.s1_posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eda7efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetacognitiveArbitrator:\n",
    "    def __init__(self, \n",
    "                 n_dims=4, \n",
    "                 n_cats=2, \n",
    "                 beta_s1=5.0, \n",
    "                 arb_threshold=2.0, \n",
    "                 arb_slope=5.0, \n",
    "                 hint_alpha=0.5, \n",
    "                 s2_params=None,\n",
    "                 s1_core=None): # [优化] 新增参数，允许传入预计算好的 S1\n",
    "        \n",
    "        # === System 1: Hypotheses Testing ===\n",
    "        # [优化] 如果传入了现成的 s1_core，就直接用，避免重复计算几何分割\n",
    "        if s1_core is not None:\n",
    "            self.s1_core = s1_core\n",
    "        else:\n",
    "            self.s1_core = Partition(n_dims, n_cats)\n",
    "            \n",
    "        self.n_hypos = self.s1_core.length\n",
    "        self.s1_posterior = np.ones(self.n_hypos) / self.n_hypos\n",
    "        self.beta_s1 = beta_s1\n",
    "        self.s1_centers = self.s1_core.get_centers() \n",
    "        \n",
    "        # === System 2: Rational Model ===\n",
    "        if s2_params is None:\n",
    "            s2_params = {'alpha': 1.0, 'sigma': 0.15, 'l0': 0.001}\n",
    "        self.s2_core = RationalModelSystem2(n_dims, **s2_params)\n",
    "        \n",
    "        # === Arbitration Parameters ===\n",
    "        self.arb_threshold = arb_threshold\n",
    "        self.arb_slope = arb_slope\n",
    "        self.hint_alpha = hint_alpha\n",
    "        \n",
    "        self.history = {'w_s1': [], 'entropy': [], 'prob_correct': [], 'human_correct': []}\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def get_s1_entropy(self):\n",
    "        return entropy(self.s1_posterior)\n",
    "\n",
    "    def calculate_arbitration_weight(self):\n",
    "        H = self.get_s1_entropy()\n",
    "        weight = self._sigmoid(-self.arb_slope * (H - self.arb_threshold))\n",
    "        return weight, H\n",
    "\n",
    "    def _compute_s2_hint_signal(self):\n",
    "        \"\"\"计算 Hint (已包含之前的 Index 修复)\"\"\"\n",
    "        s2_centers = []\n",
    "        s2_labels = []\n",
    "        for cluster in self.s2_core.clusters:\n",
    "            mu = cluster['sum_x'] / cluster['N']\n",
    "            pred_cat = max(cluster['y_counts'], key=cluster['y_counts'].get)\n",
    "            s2_centers.append(mu)\n",
    "            s2_labels.append(pred_cat - 1)\n",
    "\n",
    "        if not s2_centers:\n",
    "            return np.ones(self.n_hypos)\n",
    "\n",
    "        s2_centers = np.array(s2_centers) \n",
    "        n_clusters = len(s2_centers)\n",
    "        hint_vector = np.zeros(self.n_hypos)\n",
    "\n",
    "        dummy_data = (s2_centers, [1]*n_clusters, [1]*n_clusters)\n",
    "\n",
    "        for h in range(self.n_hypos):\n",
    "            # calc_likelihood_boundary 返回 shape: [n_cats, n_trials]\n",
    "            prob_matrix = self.s1_core.calc_likelihood_boundary(h, dummy_data, beta=self.beta_s1)\n",
    "            # 取出每个 cluster (trial) 对应其预测类别 (cat) 的概率\n",
    "            cluster_match_scores = prob_matrix[s2_labels, np.arange(n_clusters)]\n",
    "            hint_vector[h] = np.mean(cluster_match_scores)\n",
    "\n",
    "        return hint_vector\n",
    "\n",
    "    def predict(self, stimulus, return_details=False):\n",
    "        # 1. S1 预测\n",
    "        dummy_data = ([stimulus], [1], [1])\n",
    "        s1_liks = np.zeros((self.s1_core.n_cats, self.n_hypos))\n",
    "        \n",
    "        for h in range(self.n_hypos):\n",
    "            prob_matrix = self.s1_core.calc_likelihood_boundary(h, dummy_data, beta=self.beta_s1)\n",
    "            s1_liks[:, h] = prob_matrix[:, 0] # [修复] 正确的切片\n",
    "\n",
    "        p_s1 = np.dot(s1_liks, self.s1_posterior)\n",
    "        \n",
    "        # 2. S2 预测\n",
    "        p_s2_dict = self.s2_core.get_choice_probs(stimulus)\n",
    "        p_s2 = np.array([p_s2_dict.get(c+1, 0) for c in range(self.s1_core.n_cats)])\n",
    "        \n",
    "        # 3. 仲裁\n",
    "        w_s1, entropy_val = self.calculate_arbitration_weight()\n",
    "        p_final = w_s1 * p_s1 + (1 - w_s1) * p_s2\n",
    "        p_final /= np.sum(p_final)\n",
    "        \n",
    "        if return_details:\n",
    "            return p_final, {'w_s1': w_s1, 'p_s1': p_s1, 'p_s2': p_s2, 'entropy': entropy_val}\n",
    "        return p_final\n",
    "\n",
    "    def update(self, stimulus, category):\n",
    "        cat_idx = int(category) - 1\n",
    "        \n",
    "        # 1. S1 更新\n",
    "        dummy_data = ([stimulus], [1], [1])\n",
    "        likelihoods = np.zeros(self.n_hypos)\n",
    "        \n",
    "        for h in range(self.n_hypos):\n",
    "            prob_matrix = self.s1_core.calc_likelihood_boundary(h, dummy_data, beta=self.beta_s1)\n",
    "            likelihoods[h] = prob_matrix[cat_idx, 0] # [修复] 正确的切片\n",
    "        \n",
    "        self.s1_posterior *= likelihoods\n",
    "        if np.sum(self.s1_posterior) > 0:\n",
    "            self.s1_posterior /= np.sum(self.s1_posterior)\n",
    "        else:\n",
    "            self.s1_posterior = np.ones(self.n_hypos) / self.n_hypos\n",
    "            \n",
    "        # 2. S2 更新\n",
    "        self.s2_core.update(stimulus, category)\n",
    "        \n",
    "        # 3. Hint 更新\n",
    "        w_s1, _ = self.calculate_arbitration_weight()\n",
    "        if w_s1 < 0.8 and self.s2_core.total_N > 5:\n",
    "            hint_vector = self._compute_s2_hint_signal()\n",
    "            self.s1_posterior *= (1 + self.hint_alpha * hint_vector)\n",
    "            if np.sum(self.s1_posterior) > 0:\n",
    "                self.s1_posterior /= np.sum(self.s1_posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83eff9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始并行拟合 8 个被试，使用 CPU 核心数: 128...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Start] Subject 22\n",
      "[Start] Subject 7\n",
      "[Start] Subject 16\n",
      "[Start] Subject 1\n",
      "[Start] Subject 19\n",
      "Loading similarity matrix from disk: /home/yangjiong/CategoryLearning_gitcode/src/Hybrid/cache/similarity_matrix_d4_c2.npy\n",
      "[Start] Subject 4\n",
      "Loading similarity matrix from disk: /home/yangjiong/CategoryLearning_gitcode/src/Hybrid/cache/similarity_matrix_d4_c2.npy\n",
      "Loading similarity matrix from disk: /home/yangjiong/CategoryLearning_gitcode/src/Hybrid/cache/similarity_matrix_d4_c2.npy\n",
      "Loading similarity matrix from disk: /home/yangjiong/CategoryLearning_gitcode/src/Hybrid/cache/similarity_matrix_d4_c2.npy\n",
      "Loading similarity matrix from disk: /home/yangjiong/CategoryLearning_gitcode/src/Hybrid/cache/similarity_matrix_d4_c2.npy\n",
      "Loading similarity matrix from disk: /home/yangjiong/CategoryLearning_gitcode/src/Hybrid/cache/similarity_matrix_d4_c2.npy\n",
      "[Start] Subject 10\n",
      "Loading similarity matrix from disk: /home/yangjiong/CategoryLearning_gitcode/src/Hybrid/cache/similarity_matrix_d4_c2.npy\n",
      "[Start] Subject 13\n",
      "Loading similarity matrix from disk: /home/yangjiong/CategoryLearning_gitcode/src/Hybrid/cache/similarity_matrix_d4_c2.npy\n",
      "[Done] Subject 1 (NLL: 91.49)\n",
      "[Done] Subject 4 (NLL: 88.51)\n",
      "[Done] Subject 13 (NLL: 176.53)\n",
      "[Done] Subject 19 (NLL: 131.73)\n",
      "[Done] Subject 7 (NLL: 175.01)\n",
      "[Done] Subject 10 (NLL: 108.42)\n",
      "[Done] Subject 16 (NLL: 286.65)\n",
      "[Done] Subject 22 (NLL: 200.93)\n",
      "拟合完成，参数已保存。\n",
      "开始生成绘图...\n",
      "所有任务完成！\n"
     ]
    }
   ],
   "source": [
    "def neg_log_likelihood(params, df_sub, s1_core_shared):\n",
    "    \"\"\"\n",
    "    NLL 计算函数\n",
    "    s1_core_shared: 预先初始化好的 Partition 对象，避免重复计算\n",
    "    \"\"\"\n",
    "    beta_s1, arb_threshold, s2_alpha, s2_sigma = params\n",
    "    \n",
    "    # 传递 s1_core_shared 进模型\n",
    "    model = MetacognitiveArbitrator(\n",
    "        n_dims=4, n_cats=2,\n",
    "        beta_s1=beta_s1,\n",
    "        arb_threshold=arb_threshold,\n",
    "        arb_slope=5.0, \n",
    "        hint_alpha=0.5,\n",
    "        s2_params={'alpha': s2_alpha, 'sigma': s2_sigma, 'l0': 0.001},\n",
    "        s1_core=s1_core_shared # 关键优化\n",
    "    )\n",
    "    \n",
    "    nll = 0.0\n",
    "    for _, row in df_sub.iterrows():\n",
    "        x = np.array([row['feature1'], row['feature2'], row['feature3'], row['feature4']])\n",
    "        choice = int(row['choice'])\n",
    "        true_cat = int(row['category'])\n",
    "        \n",
    "        probs = model.predict(x)\n",
    "        p_choice = probs[choice-1]\n",
    "        p_choice = max(p_choice, 1e-6)\n",
    "        nll -= np.log(p_choice)\n",
    "        model.update(x, true_cat)\n",
    "        \n",
    "    return nll\n",
    "\n",
    "def fit_single_subject(sub_id, sub_df):\n",
    "    \"\"\"\n",
    "    单个被试的完整拟合流程，将被 Parallel 调用\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"[Start] Subject {sub_id}\")\n",
    "        \n",
    "        # 在每个进程内初始化一次 s1_core，供该进程内的 minimize 多次调用\n",
    "        # 虽然每个进程都会初始化一次 Partition，但比 minimize 每次迭代都初始化要快几百倍\n",
    "        s1_core_local = Partition(n_dims=4, n_cats=2)\n",
    "        \n",
    "        # 初始参数 [beta_s1, arb_threshold, s2_alpha, s2_sigma]\n",
    "        x0 = [5.0, 2.0, 1.0, 0.15]\n",
    "        bounds = [(0.1, 20), (0.1, 5), (0.1, 5), (0.01, 0.5)]\n",
    "        \n",
    "        # 运行优化\n",
    "        res = minimize(\n",
    "            neg_log_likelihood, \n",
    "            x0, \n",
    "            args=(sub_df, s1_core_local), # 传入 s1_core\n",
    "            bounds=bounds, \n",
    "            method='L-BFGS-B'\n",
    "        )\n",
    "        \n",
    "        # 记录参数结果\n",
    "        param_result = {\n",
    "            'iSub': sub_id,\n",
    "            'beta_s1': res.x[0],\n",
    "            'arb_threshold': res.x[1],\n",
    "            's2_alpha': res.x[2],\n",
    "            's2_sigma': res.x[3],\n",
    "            'nll': res.fun\n",
    "        }\n",
    "        \n",
    "        # === 重新运行一次以获取历史数据 (用于画图) ===\n",
    "        best_model = MetacognitiveArbitrator(\n",
    "            n_dims=4, n_cats=2,\n",
    "            beta_s1=res.x[0],\n",
    "            arb_threshold=res.x[1],\n",
    "            arb_slope=5.0, hint_alpha=0.5,\n",
    "            s2_params={'alpha': res.x[2], 'sigma': res.x[3], 'l0': 0.001},\n",
    "            s1_core=s1_core_local\n",
    "        )\n",
    "        \n",
    "        history = {'iSub': [], 'trial': [], 'w_s1': [], 'entropy': [], 'prob_correct': [], 'human_correct': []}\n",
    "        for i, row in sub_df.iterrows():\n",
    "            x = np.array([row['feature1'], row['feature2'], row['feature3'], row['feature4']])\n",
    "            true_cat = int(row['category'])\n",
    "            choice = int(row['choice'])\n",
    "            \n",
    "            probs, details = best_model.predict(x, return_details=True)\n",
    "            best_model.update(x, true_cat)\n",
    "            \n",
    "            history['iSub'].append(sub_id)\n",
    "            history['trial'].append(i+1)\n",
    "            history['w_s1'].append(details['w_s1'])\n",
    "            history['entropy'].append(details['entropy'])\n",
    "            history['prob_correct'].append(probs[true_cat-1])\n",
    "            history['human_correct'].append(1 if choice == true_cat else 0)\n",
    "        \n",
    "        print(f\"[Done] Subject {sub_id} (NLL: {res.fun:.2f})\")\n",
    "        return param_result, pd.DataFrame(history)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Subject {sub_id}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ==========================================\n",
    "# 3. 主程序：并行调度\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    if not os.path.exists('df_con1.csv'):\n",
    "        print(\"错误：找不到 df_con1.csv\")\n",
    "    else:\n",
    "        df = pd.read_csv('df_con1.csv')\n",
    "        sub_ids = df['iSub'].unique()\n",
    "        \n",
    "        os.makedirs('plots_hybrid', exist_ok=True)\n",
    "        \n",
    "        # === 并行执行 ===\n",
    "        # n_jobs=-1 表示使用所有可用的 CPU 核心\n",
    "        print(f\"开始并行拟合 {len(sub_ids)} 个被试，使用 CPU 核心数: {os.cpu_count()}...\")\n",
    "        \n",
    "        results_list = Parallel(n_jobs=-1)(\n",
    "            delayed(fit_single_subject)(sub, df[df['iSub'] == sub].copy().reset_index(drop=True))\n",
    "            for sub in sub_ids\n",
    "        )\n",
    "        \n",
    "        # === 整理结果 ===\n",
    "        all_params = []\n",
    "        all_histories = []\n",
    "        \n",
    "        for param, hist in results_list:\n",
    "            if param is not None:\n",
    "                all_params.append(param)\n",
    "                all_histories.append(hist)\n",
    "        \n",
    "        # 保存参数\n",
    "        params_df = pd.DataFrame(all_params)\n",
    "        params_df.to_csv('fitted_params_hybrid_v2.csv', index=False)\n",
    "        print(\"拟合完成，参数已保存。\")\n",
    "        \n",
    "        # === 绘图 (串行绘图即可，速度很快) ===\n",
    "        print(\"开始生成绘图...\")\n",
    "        for hist_df in all_histories:\n",
    "            sub = hist_df['iSub'].iloc[0]\n",
    "            \n",
    "            fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "            \n",
    "            # 左轴：权重\n",
    "            color = 'tab:blue'\n",
    "            ax1.set_xlabel('Trial')\n",
    "            ax1.set_ylabel('Reliance on Rule (Weight S1)', color=color, fontweight='bold')\n",
    "            ax1.plot(hist_df['trial'], hist_df['w_s1'], color=color, linewidth=2, label='Weight S1')\n",
    "            ax1.tick_params(axis='y', labelcolor=color)\n",
    "            ax1.set_ylim(-0.05, 1.05)\n",
    "            \n",
    "            # 右轴：熵\n",
    "            ax2 = ax1.twinx()\n",
    "            color = 'tab:red'\n",
    "            ax2.set_ylabel('Rule Uncertainty (S1 Entropy)', color=color, fontweight='bold')\n",
    "            ax2.plot(hist_df['trial'], hist_df['entropy'], color=color, linestyle='--', linewidth=2, label='Entropy')\n",
    "            ax2.tick_params(axis='y', labelcolor=color)\n",
    "            \n",
    "            plt.title(f'Subject {sub}: Cognitive Control Dynamics')\n",
    "            fig.tight_layout()\n",
    "            plt.savefig(f'plots_hybrid_v2/sub_{sub}_dynamics.png')\n",
    "            plt.close()\n",
    "            \n",
    "        print(\"所有任务完成！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
