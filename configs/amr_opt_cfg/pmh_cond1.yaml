# AMR optimization config for Condition 1 subjects (old config replica)
# Uses original_strategies_a (top_posterior, max 4, no ksimilar)

engine_config:
  modules:
    perception_mod:
      class: 'src.Bayesian_state.problems.modules.perception.PerceptionModule'
    hypo_transitions_mod:
      class: 'src.Bayesian_state.problems.modules.hypo_transitions.DynamicHypothesisModule'
      kwargs:
        # Use predefined Condition 1 strategy (Entropy-based, more stable than random)
        strategies: "original_strategies_a"
        init_num: 3
    likelihood_mod:
      class: 'src.Bayesian_state.problems.modules.likelihood.LikelihoodModule'
      kwargs:
        beta: None
    memory_mod:
      class: 'src.Bayesian_state.problems.modules.memory.DualMemoryModule'
      kwargs:
        w0: None
        gamma: None
  agenda:
    - perception_mod
    - hypo_transitions_mod
    - likelihood_mod
    - memory_mod

# Set subjects after running list_conditions.py; example:
# subjects: [1, 4, 7, 10, 13, 16, 19, 22]
subjects: [1, 4, 7, 10, 13, 16, 19, 22]
# or use subject_range: [1, 24]

# Paths (relative to this YAML)
data_path: ../../data/processed/Task2_processed.csv
output_dir: ../../results/state-based-AMR-result/pmh/cond1

# Parallelism
# Run all 8 subjects in parallel and give each 12 inner workers (â‰ˆ96 cores total)
n_jobs_subjects: 8
n_jobs_inner: 12

# Repeats
n_repeats: 12
refit_repeats: 128

# Evaluation
# Per-subject window sizes align with the subjects order above (Increased to 16 for smoother plots)
window_size: [16, 16, 16, 16, 16, 16, 16, 16]
stop_at: 1.0
max_trials: null
keep_logs: true

param_grid:
  gamma: [0.05, 0.999]
  w0: [0.01, 1.0]
  beta: [1.0, 100.0]

amr_kwargs:
  max_evals: 512
  coarse_grid_per_dim: 3
  split_factor: 2
  refine_top_k: 3
