{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Modeling ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定项目根目录\n",
    "project_root = Path(os.getcwd()).parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入数据\n",
    "processed_path = Path(project_root) / 'data' / 'processed'\n",
    "learning_data = pd.read_csv(processed_path / 'Task2_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模型\n",
    "from src.Bayesian_new import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Unit Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 0.1 Base Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Partition(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.calc_likelihood_base(5, (np.array([[0.1,0.2,0.3,0.4],[0.1,0.4,0.3,0.2],[0.4,0.2,0.3,0.1]]), np.array([3,2,3]), np.array([1,0,1])), 2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.calc_likelihood_entry(5, (np.array([[0.1,0.2,0.3,0.4],[0.1,0.4,0.3,0.2],[0.4,0.2,0.3,0.1]]), np.array([3,2,3]), np.array([1,0,1])), 2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.calc_trueprob_entry(5, (np.array([[0.1,0.2,0.3,0.4],[0.1,0.4,0.3,0.2],[0.4,0.2,0.3,0.1]]), np.array([3,2,3]), np.array([1,0,1]), np.array([3,1,1])), 2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.calc_likelihood_entry(2, (np.array([[0.1,0.2,0.3,0.4],[0.1,0.2,0.3,0.4],[0.1,0.2,0.3,0.4]]), np.array([3,2,3]), np.array([1,0,1])), 2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = PartitionLikelihood(BaseSet(range(10)), p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.h_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.get_likelihood((np.array([[0.1,0.2,0.3,0.4],[0.05,0.02,0.53,0.4],[0.1,0.2,0.3,0.4]]), np.array([3,2,3]), np.array([1,1,0])), beta=15., normalized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl = SoftPartitionLikelihood(BaseSet(range(10)), p, np.exp(np.linspace(0,5,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spl.get_likelihood((np.array([[0.1,0.2,0.3,0.4],[0.05,0.02,0.53,0.4],[0.1,0.2,0.3,0.4]]), np.array([3,2,3]), np.array([1,1,0])), normalized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2 Module Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Base model (oral generated k-cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.Bayesian_new.problems.model_v1 as model\n",
    "importlib.reload(model)\n",
    "from src.Bayesian_new.problems.model_v1 import SingleRationalModel\n",
    "\n",
    "import src.Bayesian_new.problems.config as config\n",
    "importlib.reload(config)\n",
    "from src.Bayesian_new.problems.config import config_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型拟合\n",
    "fitting_results = {}\n",
    "limited_hypos_list = {}\n",
    "\n",
    "for i, (iSub, subject_data) in enumerate(learning_data.groupby('iSub')):\n",
    "    condition = subject_data['condition'].iloc[0]\n",
    "    model_base = SingleRationalModel(config_base, condition=condition)\n",
    "    s_data = (subject_data[[\"feature1\", \"feature2\", \"feature3\", \"feature4\"]].values,\n",
    "            subject_data[\"choice\"].values,\n",
    "            subject_data[\"feedback\"].values)\n",
    "\n",
    "    # 假设全集\n",
    "    # limited_hypos_list = None\n",
    "\n",
    "    # 口头汇报限制假设集\n",
    "    data_1 = (subject_data[[\"feature1_oral\", \"feature2_oral\", \"feature3_oral\", \"feature4_oral\"]].values,\n",
    "            subject_data[\"choice\"].values)\n",
    "    limited_hypos_list[iSub] = model_base.oral_generate_hypos(data_1)\n",
    "\n",
    "    step_results = model_base.fit_step_by_step(s_data, limited_hypos_list)\n",
    "    fitting_results[iSub] = {\n",
    "        'iSub': iSub,\n",
    "        'condition': condition,\n",
    "        'step_results': step_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_results = fitting_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_results = sub_results['step_results']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Base model (model generated k-cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.Bayesian_new.problems.model_v2 as model\n",
    "importlib.reload(model)\n",
    "from src.Bayesian_new.problems.model_v2 import SingleRationalModel\n",
    "\n",
    "import src.Bayesian_new.problems.config as config\n",
    "importlib.reload(config)\n",
    "from src.Bayesian_new.problems.config import config_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型拟合\n",
    "fitting_results = {}\n",
    "\n",
    "for i, (iSub, subject_data) in enumerate(learning_data.groupby('iSub')):\n",
    "    if iSub==1:\n",
    "        condition = subject_data['condition'].iloc[0]\n",
    "        model_base = SingleRationalModel(config_base, condition=condition)\n",
    "        s_data = (subject_data[[\"feature1\", \"feature2\", \"feature3\", \"feature4\"]].values,\n",
    "                subject_data[\"choice\"].values,\n",
    "                subject_data[\"feedback\"].values)\n",
    "\n",
    "        step_results = model_base.fit_step_by_step(s_data, dynamic_limit=True)\n",
    "        fitting_results[iSub] = {\n",
    "            'iSub': iSub,\n",
    "            'condition': condition,\n",
    "            'step_results': step_results\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取 step_results 的 hypo_details 的 key 值\n",
    "step_keys_list = []\n",
    "\n",
    "for iSub, sub_result in fitting_results.items():\n",
    "    step_results = sub_result['step_results']\n",
    "    for step in step_results:\n",
    "        hypo_details = step.get('hypo_details', {})\n",
    "        step_keys_list.append(list(hypo_details.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.1) cluster transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.Bayesian_new.problems.model as model\n",
    "importlib.reload(model)\n",
    "from src.Bayesian_new.problems.model import SingleRationalModel\n",
    "\n",
    "import src.Bayesian_new.problems.config as config\n",
    "importlib.reload(config)\n",
    "from src.Bayesian_new.problems import hypo_transitions as ht\n",
    "importlib.reload(ht)\n",
    "from src.Bayesian_new.problems.config import config_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型拟合\n",
    "fitting_results = {}\n",
    "\n",
    "for i, (iSub, subject_data) in enumerate(learning_data.groupby('iSub')):\n",
    "    if iSub==2:\n",
    "        condition = subject_data['condition'].iloc[0]\n",
    "        s_data = (subject_data[[\"feature1\", \"feature2\", \"feature3\", \"feature4\"]].values,\n",
    "                subject_data[\"choice\"].values,\n",
    "                subject_data[\"feedback\"].values)\n",
    "        \n",
    "        transition_spec=[(1, \"top_posterior\"), (2, \"ksimilar_centers\"), (4, \"random\")]\n",
    "        n_dims = 4\n",
    "        n_cats = 2 if condition==1 else 4\n",
    "        partition = ht.PartitionCluster(n_dims, n_cats, transition_spec=transition_spec)    \n",
    "        model_base = SingleRationalModel(config_base, condition=condition, partition=partition)\n",
    "        \n",
    "        step_results = model_base.fit_step_by_step(s_data, cluster=True,\n",
    "                                        cluster_prototype_amount=1,\n",
    "                                        cluster_kwargs={\"cluster_hypo_method\":\"random\",\n",
    "                                                        \"proto_hypo_method\":\"random\"})\n",
    "        fitting_results[iSub] = {\n",
    "            'iSub': iSub,\n",
    "            'condition': condition,\n",
    "            'step_results': step_results\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取 step_results 的 hypo_details 的 key 值\n",
    "step_keys_list = []\n",
    "\n",
    "for iSub, sub_result in fitting_results.items():\n",
    "    step_results = sub_result['step_results']\n",
    "    for step in step_results:\n",
    "        hypo_details = step.get('hypo_details', {})\n",
    "        step_keys_list.append(list(hypo_details.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Forget model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.Bayesian_new.problems.forget as forget\n",
    "importlib.reload(forget)\n",
    "from src.Bayesian_new.problems.forget import ForgetModel\n",
    "\n",
    "import src.Bayesian_new.problems.config as config\n",
    "importlib.reload(config)\n",
    "from src.Bayesian_new.problems.config import config_fgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型拟合\n",
    "fitting_results = {}\n",
    "predict_results = {}\n",
    "\n",
    "for i, (iSub, subject_data) in enumerate(learning_data.groupby('iSub')):\n",
    "    if iSub==10:\n",
    "        condition = subject_data['condition'].iloc[0]\n",
    "        model_fgt = ForgetModel(config_fgt, condition=condition)\n",
    "        s_data = (subject_data[[\"feature1\", \"feature2\", \"feature3\", \"feature4\"]].values,\n",
    "                subject_data[\"choice\"].values,\n",
    "                subject_data[\"feedback\"].values, \n",
    "                subject_data[\"category\"].values)\n",
    "        \n",
    "        optimize_results = model_fgt.optimize_params(s_data)\n",
    "        fitting_results[iSub] = {\n",
    "            'iSub': iSub,\n",
    "            'condition': condition,\n",
    "            'optimize_results': optimize_results\n",
    "        }\n",
    "        best_step_results = optimize_results['best_step_results']\n",
    "        results = model_fgt.predict_choice(s_data, best_step_results, use_cached_dist=False, window_size=16)\n",
    "\n",
    "        # step_results = model_fgt.fit_step_by_step(s_data, gamma=0.4, w0=0.001)\n",
    "        # fitting_results[iSub] = {\n",
    "        #     'iSub': iSub,\n",
    "        #     'condition': condition,\n",
    "        #     'step_results': step_results\n",
    "        # }\n",
    "        # results = model_fgt.predict_choice(s_data, step_results, use_cached_dist=False, window_size=16)\n",
    "\n",
    "        predict_results[iSub] = {\n",
    "            'condition': condition,\n",
    "            'true_acc': results['true_acc'],\n",
    "            'pred_acc': results['pred_acc'],\n",
    "            'sliding_true_acc': results['sliding_true_acc'],\n",
    "            'sliding_pred_acc': results['sliding_pred_acc'],\n",
    "            'sliding_pred_acc_std': results['sliding_pred_acc_std']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_results = fitting_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_results = sub_results['optimize_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iSub = 1\n",
    "sliding_pred_acc = np.array(predict_results[iSub]['sliding_pred_acc'])\n",
    "sliding_true_acc = np.array(predict_results[iSub]['sliding_true_acc'])\n",
    "sliding_pred_acc_std = np.array(predict_results[iSub]['sliding_pred_acc_std'])\n",
    "condition = predict_results[iSub].get('condition', 'Unknown')\n",
    "\n",
    "plt.plot(sliding_pred_acc, label='Predicted Accuracy', color='blue')\n",
    "lower_bound = np.array(sliding_pred_acc) - np.array(sliding_pred_acc_std)\n",
    "upper_bound = np.array(sliding_pred_acc) + np.array(sliding_pred_acc_std)\n",
    "plt.fill_between(range(len(sliding_pred_acc)), lower_bound, upper_bound, color='blue', alpha=0.2, label='Predicted Accuracy ± Std')\n",
    "plt.plot(sliding_true_acc, label='True Accuracy', color='orange')\n",
    "\n",
    "ax = plt.gca()  # Get the current axes\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title(f'Subject {iSub} (Condition {condition})')\n",
    "ax.set_xlabel('Trial')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_results = fitting_results[iSub]\n",
    "grid_errors = sub_results['grid_errors']\n",
    "condition = sub_results['condition']\n",
    "\n",
    "# 提取 gamma 和 w0 的取值\n",
    "gamma_values = sorted(set(key[0] for key in grid_errors.keys()))\n",
    "w0_values = sorted(set(key[1] for key in grid_errors.keys()))\n",
    "\n",
    "# 创建 error 矩阵\n",
    "error_matrix = np.zeros((len(gamma_values), len(w0_values)))\n",
    "for (gamma, w0), error in grid_errors.items():\n",
    "    gamma_idx = gamma_values.index(gamma)\n",
    "    w0_idx = w0_values.index(w0)\n",
    "    error_matrix[gamma_idx, w0_idx] = error\n",
    "\n",
    "# 创建 grid plot\n",
    "ax = plt.gca()\n",
    "cax = ax.matshow(error_matrix, cmap='viridis_r')\n",
    "plt.colorbar(cax, ax=ax)\n",
    "\n",
    "ax.set_xticks(range(len(w0_values)))\n",
    "ax.set_yticks(range(len(gamma_values)))\n",
    "ax.set_xticklabels([f'{w0:.5f}' for w0 in w0_values], rotation=90)\n",
    "ax.set_yticklabels([f'{gamma:.3f}' for gamma in gamma_values])\n",
    "\n",
    "ax.set_title(f'Subject {iSub} (Condition {condition})')\n",
    "ax.set_xlabel('w0')\n",
    "ax.set_ylabel('Gamma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3.1) forget + k-cluster transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.Bayesian_new.problems.forget as forget\n",
    "importlib.reload(forget)\n",
    "from src.Bayesian_new.problems.forget import ForgetModel\n",
    "\n",
    "import src.Bayesian_new.problems.config as config\n",
    "importlib.reload(config)\n",
    "from src.Bayesian_new.problems import hypo_transitions as ht\n",
    "importlib.reload(ht)\n",
    "from src.Bayesian_new.problems.config import config_fgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型拟合\n",
    "fitting_results = {}\n",
    "predict_results = {}\n",
    "\n",
    "for i, (iSub, subject_data) in enumerate(learning_data.groupby('iSub')):\n",
    "    if iSub==10:\n",
    "        condition = subject_data['condition'].iloc[0]\n",
    "        model_fgt = ForgetModel(config_fgt, condition=condition)\n",
    "        s_data = (subject_data[[\"feature1\", \"feature2\", \"feature3\", \"feature4\"]].values,\n",
    "                subject_data[\"choice\"].values,\n",
    "                subject_data[\"feedback\"].values, \n",
    "                subject_data[\"category\"].values)\n",
    "        \n",
    "        transition_spec=[(1, \"top_posterior\"), (2, \"ksimilar_centers\"), (4, \"random\")]\n",
    "        n_dims = 4\n",
    "        n_cats = 2 if condition==1 else 4\n",
    "        partition = ht.PartitionCluster(n_dims, n_cats, transition_spec=transition_spec)    \n",
    "        model_fgt = ForgetModel(config_fgt, condition=condition, partition=partition)\n",
    "        \n",
    "        step_results = model_fgt.optimize_params(s_data, cluster=True,\n",
    "                                        cluster_prototype_amount=1,\n",
    "                                        cluster_kwargs={\"cluster_hypo_method\":\"random\",\n",
    "                                                        \"proto_hypo_method\":\"random\"})\n",
    "\n",
    "        fitting_results[iSub] = {\n",
    "            'iSub': iSub,\n",
    "            'condition': condition,\n",
    "            'optimize_results': optimize_results\n",
    "        }\n",
    "        best_step_results = optimize_results['best_step_results']\n",
    "        results = model_fgt.predict_choice(s_data, best_step_results, use_cached_dist=False, window_size=16)\n",
    "\n",
    "        predict_results[iSub] = {\n",
    "            'condition': condition,\n",
    "            'true_acc': results['true_acc'],\n",
    "            'pred_acc': results['pred_acc'],\n",
    "            'sliding_true_acc': results['sliding_true_acc'],\n",
    "            'sliding_pred_acc': results['sliding_pred_acc'],\n",
    "            'sliding_pred_acc_std': results['sliding_pred_acc_std']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) Adaptive amnesia model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.Bayesian_new.problems.forget as forget\n",
    "importlib.reload(forget)\n",
    "from src.Bayesian_new.problems.forget import AdaptiveAmnesiaModel\n",
    "\n",
    "import src.Bayesian_new.problems.config as config\n",
    "importlib.reload(config)\n",
    "from src.Bayesian_new.problems.config import config_fgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载拟合结果\n",
    "result_path = Path(project_root) / 'results' / 'Bayesian_new'\n",
    "forget_fitting_results = joblib.load(result_path / 'M_Fgt_fitting_results_400.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型拟合\n",
    "fitting_results = {}\n",
    "predict_results = {}\n",
    "\n",
    "for i, (iSub, subject_data) in enumerate(learning_data.groupby('iSub')):\n",
    "    if iSub==2:\n",
    "        condition = subject_data['condition'].iloc[0]\n",
    "        forget_sub_results = forget_fitting_results[iSub]\n",
    "        base_gamma = forget_sub_results['best_params'][0]\n",
    "        base_w0 = forget_sub_results['best_params'][1]\n",
    "        model_amn = AdaptiveAmnesiaModel(config_fgt, condition=condition, \n",
    "                                         base_gamma=base_gamma, base_w0=base_w0)\n",
    "        print(iSub)\n",
    "        s_data = (subject_data[[\"feature1\", \"feature2\", \"feature3\", \"feature4\"]].values,\n",
    "                subject_data[\"choice\"].values,\n",
    "                subject_data[\"feedback\"].values, \n",
    "                subject_data[\"category\"].values)\n",
    "        optimize_results = model_amn.optimize_params(s_data)\n",
    "        best_step_results = optimize_results['best_step_results']\n",
    "        results = model_amn.predict_choice(s_data, best_step_results, use_cached_dist=False, window_size=16)\n",
    "        fitting_results[iSub] = {\n",
    "            'iSub': iSub,\n",
    "            'condition': condition,\n",
    "            'optimize_results': optimize_results\n",
    "        }\n",
    "\n",
    "        predict_results[iSub] = {\n",
    "            'condition': condition,\n",
    "            'true_acc': results['true_acc'],\n",
    "            'pred_acc': results['pred_acc'],\n",
    "            'sliding_true_acc': results['sliding_true_acc'],\n",
    "            'sliding_pred_acc': results['sliding_pred_acc'],\n",
    "            'sliding_pred_acc_std': results['sliding_pred_acc_std']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#画出best_step_results的gamma_list和w0_list的变化图\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "last_step_results = best_step_results[-1]\n",
    "gamma_list = last_step_results['gamma_list']\n",
    "w0_list = last_step_results['w0_list']\n",
    "plt.plot(gamma_list)\n",
    "plt.plot(w0_list)\n",
    "plt.legend(['gamma', 'w0'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iSub = 1\n",
    "sliding_pred_acc = np.array(predict_results[iSub]['sliding_pred_acc'])\n",
    "sliding_true_acc = np.array(predict_results[iSub]['sliding_true_acc'])\n",
    "sliding_pred_acc_std = np.array(predict_results[iSub]['sliding_pred_acc_std'])\n",
    "condition = predict_results[iSub].get('condition', 'Unknown')\n",
    "\n",
    "plt.plot(sliding_pred_acc, label='Predicted Accuracy', color='blue')\n",
    "lower_bound = np.array(sliding_pred_acc) - np.array(sliding_pred_acc_std)\n",
    "upper_bound = np.array(sliding_pred_acc) + np.array(sliding_pred_acc_std)\n",
    "plt.fill_between(range(len(sliding_pred_acc)), lower_bound, upper_bound, color='blue', alpha=0.2, label='Predicted Accuracy ± Std')\n",
    "plt.plot(sliding_true_acc, label='True Accuracy', color='orange')\n",
    "\n",
    "ax = plt.gca()  # Get the current axes\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title(f'Subject {iSub} (Condition {condition})')\n",
    "ax.set_xlabel('Trial')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. batch processing ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Base (oral generated k-cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.Bayesian_new.problems.model_v1 as model\n",
    "importlib.reload(model)\n",
    "from src.Bayesian_new.problems.model_v1 import SingleRationalModel\n",
    "\n",
    "import src.Bayesian_new.problems.config as config\n",
    "importlib.reload(config)\n",
    "from src.Bayesian_new.problems.config import config_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 封装单个被试的拟合过程\n",
    "def process_single_subject(iSub, subject_data, config, limit):\n",
    "    condition = subject_data['condition'].iloc[0]\n",
    "    model_base = SingleRationalModel(config, condition=condition)\n",
    "    \n",
    "    if limit:\n",
    "        data_1 = (subject_data[[\"feature1_oral\", \"feature2_oral\", \"feature3_oral\", \"feature4_oral\"]].values,\n",
    "                  subject_data[\"choice\"].values)\n",
    "        limited_hypos_list = model_base.oral_generate_hypos(data_1)\n",
    "    else:\n",
    "        limited_hypos_list = None\n",
    "\n",
    "    s_data = (subject_data[[\"feature1\", \"feature2\", \"feature3\", \"feature4\"]].values,\n",
    "              subject_data[\"choice\"].values,\n",
    "              subject_data[\"feedback\"].values)\n",
    "    step_results = model_base.fit_step_by_step(s_data, limited_hypos_list)\n",
    "\n",
    "    return {\n",
    "        'iSub': iSub,\n",
    "        'condition': condition,\n",
    "        'step_results': step_results\n",
    "    }\n",
    "\n",
    "# 并行拟合主流程\n",
    "def parallel_processing(learning_data, config, limit, n_jobs):\n",
    "    \"\"\"并行拟合所有被试\"\"\"\n",
    "    subjects = list(learning_data.groupby('iSub'))\n",
    "    \n",
    "    # 使用并行计算\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_single_subject)(iSub, sub_data, config, limit)\n",
    "        for iSub, sub_data in tqdm(subjects, desc=\"Processing Subjects\")\n",
    "    )\n",
    "    \n",
    "    # 整理结果到字典\n",
    "    fitting_results = {res['iSub']: res for res in results}\n",
    "    return fitting_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subjects: 100%|██████████| 24/24 [00:05<00:00,  4.21it/s]\n",
      "100%|██████████| 128/128 [00:01<00:00, 72.96it/s]\n",
      "100%|██████████| 128/128 [00:01<00:00, 71.50it/s]\n",
      "100%|██████████| 192/192 [00:02<00:00, 76.88it/s]\n",
      "100%|██████████| 192/192 [00:02<00:00, 75.59it/s]\n",
      "100%|██████████| 256/256 [00:03<00:00, 73.32it/s]\n",
      "100%|██████████| 256/256 [00:03<00:00, 68.83it/s]\n",
      "100%|██████████| 384/384 [00:05<00:00, 71.41it/s]\n",
      "100%|██████████| 448/448 [00:05<00:00, 75.22it/s]\n",
      "100%|██████████| 128/128 [00:08<00:00, 15.69it/s]]\n",
      "100%|██████████| 128/128 [00:09<00:00, 13.85it/s]]\n",
      "100%|██████████| 256/256 [00:18<00:00, 14.11it/s]]\n",
      "100%|██████████| 384/384 [00:25<00:00, 15.05it/s]]\n",
      "100%|██████████| 448/448 [00:32<00:00, 13.63it/s]]\n",
      "100%|██████████| 505/505 [00:38<00:00, 13.16it/s]]\n",
      "100%|██████████| 512/512 [00:38<00:00, 13.32it/s]\n",
      "100%|██████████| 512/512 [00:39<00:00, 13.05it/s]]\n",
      "100%|██████████| 640/640 [00:45<00:00, 14.18it/s]]\n",
      "100%|██████████| 640/640 [00:49<00:00, 12.94it/s]]\n",
      "100%|██████████| 639/639 [00:51<00:00, 12.53it/s]]\n",
      "100%|██████████| 640/640 [00:53<00:00, 11.98it/s]]\n",
      "100%|██████████| 704/704 [00:59<00:00, 11.93it/s]]\n",
      "100%|██████████| 704/704 [00:59<00:00, 11.92it/s]\n",
      "100%|██████████| 1088/1088 [01:37<00:00, 11.19it/s]\n",
      "100%|██████████| 1344/1344 [02:11<00:00, 10.21it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    fitting_results = parallel_processing(learning_data, config_base, limit=False, n_jobs=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/yangjiong/CategoryLearning/results/Bayesian_new/M_Base_fitting_results.joblib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存拟合结果\n",
    "result_path = Path(project_root) / 'results' / 'Bayesian_new'\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "joblib.dump(fitting_results, result_path / 'M_Base_fitting_results.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Base (model generated k-cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.Bayesian_new.problems.model as model\n",
    "importlib.reload(model)\n",
    "from src.Bayesian_new.problems.model import SingleRationalModel\n",
    "\n",
    "import src.Bayesian_new.problems.config as config\n",
    "importlib.reload(config)\n",
    "from src.Bayesian_new.problems.config import config_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 封装单个被试的拟合过程\n",
    "def process_single_subject(iSub, subject_data, config, dynamic_limit):\n",
    "    condition = subject_data['condition'].iloc[0]\n",
    "    model_base = SingleRationalModel(config, condition=condition)\n",
    "\n",
    "    s_data = (subject_data[[\"feature1\", \"feature2\", \"feature3\", \"feature4\"]].values,\n",
    "              subject_data[\"choice\"].values,\n",
    "              subject_data[\"feedback\"].values)\n",
    "    step_results = model_base.fit_step_by_step(s_data, dynamic_limit=dynamic_limit)\n",
    "\n",
    "    return {\n",
    "        'iSub': iSub,\n",
    "        'condition': condition,\n",
    "        'step_results': step_results\n",
    "    }\n",
    "\n",
    "# 并行拟合主流程\n",
    "def parallel_processing(learning_data, config, dynamic_limit, n_jobs):\n",
    "    \"\"\"并行拟合所有被试\"\"\"\n",
    "    subjects = list(learning_data.groupby('iSub'))\n",
    "    \n",
    "    # 使用并行计算\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_single_subject)(iSub, sub_data, config, dynamic_limit)\n",
    "        for iSub, sub_data in tqdm(subjects, desc=\"Processing Subjects\")\n",
    "    )\n",
    "    \n",
    "    # 整理结果到字典\n",
    "    fitting_results = {res['iSub']: res for res in results}\n",
    "    return fitting_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    fitting_results = parallel_processing(learning_data, config_base, dynamic_limit=True, n_jobs=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/yangjiong/CategoryLearning/results/Bayesian_new/M_Fgt_fitting_results_400.joblib']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 保存拟合结果\n",
    "result_path = Path(project_root) / 'results' / 'Bayesian_new'\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "joblib.dump(fitting_results, result_path / 'M_Base_fitting_results.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载拟合结果\n",
    "result_path = Path(project_root) / 'results' / 'Bayesian_new'\n",
    "fitting_results = joblib.load(result_path / 'M_Base_fitting_results.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对比模型预测和实际数据\n",
    "predict_results = {}\n",
    "\n",
    "for i, (iSub, subject_data) in enumerate(learning_data.groupby('iSub')):\n",
    "    condition = subject_data['condition'].iloc[0]\n",
    "    model_base = SingleRationalModel(config_base, condition=condition)\n",
    "    s_data = (subject_data[[\"feature1\", \"feature2\", \"feature3\", \"feature4\"]].values,\n",
    "            subject_data[\"choice\"].values,\n",
    "            subject_data[\"feedback\"].values, \n",
    "            subject_data[\"category\"].values)\n",
    "\n",
    "    sub_results = fitting_results[iSub]\n",
    "    step_results = sub_results['step_results']\n",
    "    results = model_base.predict_choice(s_data, step_results, use_cached_dist=False, window_size=16)\n",
    "\n",
    "    predict_results[iSub] = {\n",
    "        'condition': condition,\n",
    "        'true_acc': results['true_acc'],\n",
    "        'pred_acc': results['pred_acc'],\n",
    "        'sliding_true_acc': results['sliding_true_acc'],\n",
    "        'sliding_pred_acc': results['sliding_pred_acc'],\n",
    "        'sliding_pred_acc_std': results['sliding_pred_acc_std']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模型分析\n",
    "import src.Bayesian_new.utils.model_evaluation as model_eval\n",
    "importlib.reload(model_eval)\n",
    "from src.Bayesian_new.utils.model_evaluation import ModelEval\n",
    "\n",
    "model_eval = ModelEval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制最优参数变化图\n",
    "result_path = Path(project_root) / 'results' / 'Bayesian_new'\n",
    "model_eval.plot_params_over_trials(fitting_results, 'best_beta', result_path / 'M_Base_cluster_beta.png')\n",
    "\n",
    "# 绘制k后验概率变化图\n",
    "model_eval.plot_posterior_probabilities(fitting_results, True, result_path / 'M_Base_cluster_posteriors.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制模型预测和实际数据对比图\n",
    "model_eval.plot_accuracy_comparison(predict_results, result_path / 'M_Base_cluster_acc.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 A Variation of k-Cluster Transition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.Bayesian_new.problems.model as model\n",
    "importlib.reload(model)\n",
    "from src.Bayesian_new.problems.model import SingleRationalModel\n",
    "\n",
    "import src.Bayesian_new.problems.config as config\n",
    "importlib.reload(config)\n",
    "from src.Bayesian_new.problems import hypo_transitions as ht\n",
    "importlib.reload(ht)\n",
    "from src.Bayesian_new.problems.config import config_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 封装单个被试的拟合过程\n",
    "def process_single_subject(iSub, subject_data, config, cluster, **kwargs):\n",
    "    condition = subject_data['condition'].iloc[0]\n",
    "    s_data = (subject_data[[\"feature1\", \"feature2\", \"feature3\", \"feature4\"]].values,\n",
    "              subject_data[\"choice\"].values,\n",
    "              subject_data[\"feedback\"].values)\n",
    "\n",
    "    transition_spec=[(1, \"top_posterior\"), (2, \"ksimilar_centers\"), (4, \"random\")]\n",
    "    n_dims = 4\n",
    "    n_cats = 2 if condition==1 else 4\n",
    "    partition = ht.PartitionCluster(n_dims, n_cats, transition_spec=transition_spec)    \n",
    "    model_base = SingleRationalModel(config, condition=condition, partition=partition)\n",
    "\n",
    "    step_results = model_base.fit_step_by_step(s_data, cluster=cluster, **kwargs)\n",
    "\n",
    "    return {\n",
    "        'iSub': iSub,\n",
    "        'condition': condition,\n",
    "        'step_results': step_results\n",
    "    }\n",
    "\n",
    "# 并行拟合主流程\n",
    "def parallel_processing(learning_data, config, cluster, n_jobs):\n",
    "    \"\"\"并行拟合所有被试\"\"\"\n",
    "    subjects = list(learning_data.groupby('iSub'))\n",
    "    \n",
    "    # 使用并行计算\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_single_subject)(iSub, sub_data, config, cluster, \n",
    "                                        cluster_prototype_amount=1,\n",
    "                                        cluster_kwargs={\"cluster_hypo_method\":\"top\",\n",
    "                                                        \"proto_hypo_method\":\"top\"}\n",
    "                                        # cluster_kwargs={\"cluster_hypo_method\":\"random\",\n",
    "                                        #                 \"proto_hypo_method\":\"random\"}\n",
    "                                                        )\n",
    "        for iSub, sub_data in tqdm(subjects, desc=\"Processing Subjects\")\n",
    "    )\n",
    "    \n",
    "    # 整理结果到字典\n",
    "    fitting_results = {res['iSub']: res for res in results}\n",
    "    return fitting_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    fitting_results = parallel_processing(learning_data, config_base, cluster=True, n_jobs=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存拟合结果\n",
    "result_path = Path(project_root) / 'results' / 'Bayesian_new'\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "joblib.dump(fitting_results, result_path / 'M_Base_cluster_v1_fitting_results.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对比模型预测和实际数据\n",
    "predict_results = {}\n",
    "\n",
    "for i, (iSub, subject_data) in enumerate(learning_data.groupby('iSub')):\n",
    "    condition = subject_data['condition'].iloc[0]\n",
    "    model_base = SingleRationalModel(config_base, condition=condition)\n",
    "    s_data = (subject_data[[\"feature1\", \"feature2\", \"feature3\", \"feature4\"]].values,\n",
    "            subject_data[\"choice\"].values,\n",
    "            subject_data[\"feedback\"].values, \n",
    "            subject_data[\"category\"].values)\n",
    "\n",
    "    sub_results = fitting_results[iSub]\n",
    "    step_results = sub_results['step_results']\n",
    "    results = model_base.predict_choice(s_data, step_results, use_cached_dist=False, window_size=16)\n",
    "\n",
    "    predict_results[iSub] = {\n",
    "        'condition': condition,\n",
    "        'true_acc': results['true_acc'],\n",
    "        'pred_acc': results['pred_acc'],\n",
    "        'sliding_true_acc': results['sliding_true_acc'],\n",
    "        'sliding_pred_acc': results['sliding_pred_acc'],\n",
    "        'sliding_pred_acc_std': results['sliding_pred_acc_std']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模型分析\n",
    "import src.Bayesian_new.utils.model_evaluation as model_eval\n",
    "importlib.reload(model_eval)\n",
    "from src.Bayesian_new.utils.model_evaluation import ModelEval\n",
    "\n",
    "model_eval = ModelEval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制最优参数变化图\n",
    "result_path = Path(project_root) / 'results' / 'Bayesian_new'\n",
    "model_eval.plot_params_over_trials(fitting_results, 'best_beta', result_path / 'M_Base_cluster_v1_beta.png')\n",
    "\n",
    "# 绘制k后验概率变化图\n",
    "model_eval.plot_posterior_probabilities(fitting_results, True, result_path / 'M_Base_cluster_v1_posteriors.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制模型预测和实际数据对比图\n",
    "model_eval.plot_accuracy_comparison(predict_results, result_path / 'M_Base_cluster_v1_acc.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Forget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.Bayesian_new.problems.forget as forget\n",
    "importlib.reload(forget)\n",
    "from src.Bayesian_new.problems.forget import ForgetModel\n",
    "\n",
    "import src.Bayesian_new.problems.config as config\n",
    "importlib.reload(config)\n",
    "from src.Bayesian_new.problems.config import config_fgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_task(iSub, subject_data, gamma, w0, config, limit):\n",
    "    condition = subject_data['condition'].iloc[0]\n",
    "    model_fgt = ForgetModel(config, condition=condition)\n",
    "    \n",
    "    if limit:\n",
    "        data_1 = (subject_data[[\"feature1_oral\", \"feature2_oral\", \"feature3_oral\", \"feature4_oral\"]].values,\n",
    "                  subject_data[\"choice\"].values)\n",
    "        hypos_list = model_fgt.oral_generate_hypos(data_1)\n",
    "    else:\n",
    "        hypos_list = None\n",
    "\n",
    "    s_data = (\n",
    "        subject_data[[\"feature1\", \"feature2\", \"feature3\", \"feature4\"]].values,\n",
    "        subject_data[\"choice\"].values,\n",
    "        subject_data[\"feedback\"].values, \n",
    "        subject_data[\"category\"].values\n",
    "    )\n",
    "    \n",
    "    # 做单次计算（在 forget.py 里写好的方法）\n",
    "    step_results, mean_error = model_fgt.compute_error_for_params(s_data, gamma, w0, hypos_list)\n",
    "    \n",
    "    return iSub, gamma, w0, mean_error, step_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_processing(learning_data, config, gamma_values, w0_values, limit, n_jobs=24):\n",
    "    \"\"\"\n",
    "    将 (被试, gamma, w0) 拆分为最小任务并行计算。\n",
    "    最终只保存：\n",
    "      - best_params: (gamma, w0) 最优组合\n",
    "      - best_error\n",
    "      - best_step_results (仅最优组合对应的 step 级结果)\n",
    "      - grid_errors: 一个 dict, key=(gamma,w0), value=error，用于查看各组合的误差\n",
    "    \"\"\"\n",
    "    # 1) 按被试分组\n",
    "    subjects = list(learning_data.groupby('iSub'))\n",
    "    subject_data_map = {iSub: df for iSub, df in subjects}\n",
    "\n",
    "    # 2) 枚举全部 (iSub, gamma, w0)\n",
    "    tasks = []\n",
    "    for iSub, df in subjects:\n",
    "        for gamma in gamma_values:\n",
    "            for w0 in w0_values:\n",
    "                tasks.append((iSub, gamma, w0))\n",
    "\n",
    "    # 3) 并行执行\n",
    "    results = Parallel(n_jobs=n_jobs, batch_size=1)(\n",
    "        delayed(process_single_task)(\n",
    "            iSub,\n",
    "            subject_data_map[iSub],\n",
    "            gamma,\n",
    "            w0,\n",
    "            config,\n",
    "            limit\n",
    "        )\n",
    "        for (iSub, gamma, w0) in tqdm(tasks, desc=\"Fitting (subject, gamma, w0)\")\n",
    "    )\n",
    "    # results 是个列表，元素形式为: (iSub, gamma, w0, mean_error, step_results)\n",
    "\n",
    "    # 4) 把结果收集到数据结构里\n",
    "    #    subject_grid_errors 用来记录对每个被试的 {(gamma, w0): error}\n",
    "    #    subject_best_combo 临时记录每个被试最优组合的 (gamma, w0, error, step_results)\n",
    "    subject_grid_errors = defaultdict(dict)\n",
    "    subject_best_combo = {}\n",
    "\n",
    "    for iSub, gamma, w0, err, step_res in results:\n",
    "        # 4.1 记录 grid_errors\n",
    "        subject_grid_errors[iSub][(gamma, w0)] = err\n",
    "\n",
    "        # 4.2 查看是否是当前最优，如果是，就更新 best\n",
    "        if iSub not in subject_best_combo:\n",
    "            subject_best_combo[iSub] = (gamma, w0, err, step_res)\n",
    "        else:\n",
    "            _, _, best_err, _ = subject_best_combo[iSub]\n",
    "            if err < best_err:\n",
    "                subject_best_combo[iSub] = (gamma, w0, err, step_res)\n",
    "\n",
    "    # 5) 最终返回的数据\n",
    "    fitting_results = {}\n",
    "    for iSub in subject_grid_errors.keys():\n",
    "        best_gamma, best_w0, best_error, best_step_results = subject_best_combo[iSub]\n",
    "        # grid_errors 这里就是简单地存 {(gamma, w0): error}\n",
    "        fitting_results[iSub] = {\n",
    "            \"iSub\": iSub,\n",
    "            \"condition\": subject_data_map[iSub]['condition'].iloc[0],\n",
    "            \"best_params\": (best_gamma, best_w0),\n",
    "            \"best_error\": best_error,\n",
    "            \"step_results\": best_step_results,\n",
    "            \"grid_errors\": subject_grid_errors[iSub]\n",
    "        }\n",
    "\n",
    "    return fitting_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Fitting (subject, gamma, w0): 100%|██████████| 9600/9600 [10:14<00:00, 15.63it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 一般来说 gamma_values 和 w0_values 来自你在 forget.py 里设定的范围\n",
    "    # 比如:\n",
    "    gamma_values = np.linspace(0.05, 1, 20)\n",
    "    # w0_values = np.linspace(0.00375, 0.075, 20)\n",
    "    w0_values = [0.15/(i+1) for i in range(20)]\n",
    "\n",
    "    #subject_data = learning_data.groupby('iSub').get_group(10)\n",
    "    fitting_results = parallel_processing(\n",
    "        learning_data=learning_data,     # 你的完整DataFrame\n",
    "        config=config_fgt,              # 之前定义好的config\n",
    "        gamma_values=gamma_values,      # gamma搜索范围\n",
    "        w0_values=w0_values,            # w0搜索范围\n",
    "        limit=True,               # 是否限制假设集\n",
    "        n_jobs=100                       # 并行数，一般用CPU核数\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/yangjiong/CategoryLearning/results/Bayesian_new/M_Fgt_cluster_fitting_results_400.joblib']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存拟合结果\n",
    "result_path = Path(project_root) / 'results' / 'Bayesian_new'\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "joblib.dump(fitting_results, result_path / 'M_Fgt_fitting_results_400.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Forget + k-Cluster Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.Bayesian_new.problems.forget as forget\n",
    "importlib.reload(forget)\n",
    "from src.Bayesian_new.problems.forget import ForgetModel\n",
    "\n",
    "import src.Bayesian_new.problems.config as config\n",
    "importlib.reload(config)\n",
    "from src.Bayesian_new.problems import hypo_transitions as ht\n",
    "importlib.reload(ht)\n",
    "from src.Bayesian_new.problems.config import config_fgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_task(iSub, subject_data, gamma, w0, config, cluster, **kwargs):\n",
    "    condition = subject_data['condition'].iloc[0]\n",
    "\n",
    "    s_data = (\n",
    "        subject_data[[\"feature1\", \"feature2\", \"feature3\", \"feature4\"]].values,\n",
    "        subject_data[\"choice\"].values,\n",
    "        subject_data[\"feedback\"].values, \n",
    "        subject_data[\"category\"].values\n",
    "    )\n",
    "    \n",
    "    transition_spec=[(3, \"top_posterior\"), (5, \"ksimilar_centers\"), (2, \"random\")]\n",
    "    n_dims = 4\n",
    "    n_cats = 2 if condition==1 else 4\n",
    "    partition = ht.PartitionCluster(n_dims, n_cats, transition_spec=transition_spec)    \n",
    "    model_fgt = ForgetModel(config, condition=condition, partition=partition)\n",
    "\n",
    "    # 做单次计算（在 forget.py 里写好的方法）\n",
    "    step_results, mean_error = model_fgt.compute_error_for_params(s_data, gamma, w0, cluster, **kwargs)\n",
    "    \n",
    "    return iSub, gamma, w0, mean_error, step_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_processing(learning_data, config, gamma_values, w0_values, cluster, n_jobs):\n",
    "    \"\"\"\n",
    "    将 (被试, gamma, w0) 拆分为最小任务并行计算。\n",
    "    最终只保存：\n",
    "      - best_params: (gamma, w0) 最优组合\n",
    "      - best_error\n",
    "      - best_step_results (仅最优组合对应的 step 级结果)\n",
    "      - grid_errors: 一个 dict, key=(gamma,w0), value=error，用于查看各组合的误差\n",
    "    \"\"\"\n",
    "    # 1) 按被试分组\n",
    "    subjects = list(learning_data.groupby('iSub'))\n",
    "    subject_data_map = {iSub: df for iSub, df in subjects}\n",
    "\n",
    "    # 2) 枚举全部 (iSub, gamma, w0)\n",
    "    tasks = []\n",
    "    for iSub, df in subjects:\n",
    "        for gamma in gamma_values:\n",
    "            for w0 in w0_values:\n",
    "                tasks.append((iSub, gamma, w0))\n",
    "\n",
    "    # 3) 并行执行\n",
    "    results = Parallel(n_jobs=n_jobs, batch_size=1)(\n",
    "        delayed(process_single_task)(\n",
    "            iSub,\n",
    "            subject_data_map[iSub],\n",
    "            gamma,\n",
    "            w0,\n",
    "            config,\n",
    "            cluster,\n",
    "            cluster_prototype_amount=1,\n",
    "            cluster_kwargs={\"cluster_hypo_method\":\"random\",\n",
    "                            \"proto_hypo_method\":\"random\"}         \n",
    "        )\n",
    "        for (iSub, gamma, w0) in tqdm(tasks, desc=\"Fitting (subject, gamma, w0)\")\n",
    "    )\n",
    "    # results 是个列表，元素形式为: (iSub, gamma, w0, mean_error, step_results)\n",
    "\n",
    "    # 4) 把结果收集到数据结构里\n",
    "    #    subject_grid_errors 用来记录对每个被试的 {(gamma, w0): error}\n",
    "    #    subject_best_combo 临时记录每个被试最优组合的 (gamma, w0, error, step_results)\n",
    "    subject_grid_errors = defaultdict(dict)\n",
    "    subject_best_combo = {}\n",
    "\n",
    "    for iSub, gamma, w0, err, step_res in results:\n",
    "        # 4.1 记录 grid_errors\n",
    "        subject_grid_errors[iSub][(gamma, w0)] = err\n",
    "\n",
    "        # 4.2 查看是否是当前最优，如果是，就更新 best\n",
    "        if iSub not in subject_best_combo:\n",
    "            subject_best_combo[iSub] = (gamma, w0, err, step_res)\n",
    "        else:\n",
    "            _, _, best_err, _ = subject_best_combo[iSub]\n",
    "            if err < best_err:\n",
    "                subject_best_combo[iSub] = (gamma, w0, err, step_res)\n",
    "\n",
    "    # 5) 最终返回的数据\n",
    "    fitting_results = {}\n",
    "    for iSub in subject_grid_errors.keys():\n",
    "        best_gamma, best_w0, best_error, best_step_results = subject_best_combo[iSub]\n",
    "        # grid_errors 这里就是简单地存 {(gamma, w0): error}\n",
    "        fitting_results[iSub] = {\n",
    "            \"condition\": subject_data_map[iSub]['condition'].iloc[0],\n",
    "            \"best_params\": (best_gamma, best_w0),\n",
    "            \"best_error\": best_error,\n",
    "            \"best_step_results\": best_step_results,\n",
    "            \"grid_errors\": subject_grid_errors[iSub]\n",
    "        }\n",
    "\n",
    "    return fitting_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 一般来说 gamma_values 和 w0_values 来自你在 forget.py 里设定的范围\n",
    "    # 比如:\n",
    "    gamma_values = np.linspace(0.05, 1, 20)\n",
    "    # w0_values = np.linspace(0.00375, 0.075, 20)\n",
    "    w0_values = [0.15/(i+1) for i in range(20)]\n",
    "\n",
    "    subject_data = learning_data.groupby('iSub').get_group(10)\n",
    "    fitting_results = parallel_processing(\n",
    "        learning_data=subject_data,     # 你的完整DataFrame\n",
    "        config=config_fgt,              # 之前定义好的config\n",
    "        gamma_values=gamma_values,      # gamma搜索范围\n",
    "        w0_values=w0_values,            # w0搜索范围\n",
    "        cluster=True,              # 是否使用cluster\n",
    "        n_jobs=100                       # 并行数，一般用CPU核数\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存拟合结果\n",
    "result_path = Path(project_root) / 'results' / 'Bayesian_new'\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "joblib.dump(fitting_results, result_path / 'M_Fgt_cluster_fitting_results.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载拟合结果\n",
    "result_path = Path(project_root) / 'results' / 'Bayesian_new'\n",
    "fitting_results = joblib.load(result_path / 'M_Fgt_cluster_fitting_results.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对比模型预测和实际数据\n",
    "predict_results = {}\n",
    "\n",
    "for i, (iSub, subject_data) in enumerate(learning_data.groupby('iSub')):\n",
    "    condition = subject_data['condition'].iloc[0]\n",
    "    model_fgt = ForgetModel(config_fgt, condition=condition)\n",
    "    s_data = (subject_data[[\"feature1\", \"feature2\", \"feature3\", \"feature4\"]].values,\n",
    "            subject_data[\"choice\"].values,\n",
    "            subject_data[\"feedback\"].values, \n",
    "            subject_data[\"category\"].values)\n",
    "\n",
    "    sub_results = fitting_results[iSub]\n",
    "    # optimize_results = sub_results['optimize_results']\n",
    "    best_step_results = sub_results['best_step_results']\n",
    "    results = model_fgt.predict_choice(s_data, best_step_results, use_cached_dist=False, window_size=16)\n",
    "\n",
    "    predict_results[iSub] = {\n",
    "        'condition': condition,\n",
    "        'true_acc': results['true_acc'],\n",
    "        'pred_acc': results['pred_acc'],\n",
    "        'sliding_true_acc': results['sliding_true_acc'],\n",
    "        'sliding_pred_acc': results['sliding_pred_acc'],\n",
    "        'sliding_pred_acc_std': results['sliding_pred_acc_std']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模型分析\n",
    "import src.Bayesian_new.utils.model_evaluation_new as model_eval\n",
    "importlib.reload(model_eval)\n",
    "from src.Bayesian_new.utils.model_evaluation_new import ModelEval\n",
    "\n",
    "model_eval = ModelEval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制最优参数变化图\n",
    "model_eval.plot_params_over_trials(fitting_results, 'best_beta', result_path / f'M_Fgt_cluster_beta.png')\n",
    "\n",
    "# 绘制k后验概率变化图\n",
    "model_eval.plot_posterior_probabilities(fitting_results, True, result_path / f'M_Fgt_cluster_posteriors.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制模型预测和实际数据对比图\n",
    "model_eval.plot_accuracy_comparison(predict_results, result_path / 'M_Fgt_cluster_acc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制error grid图\n",
    "model_eval.plot_error_grids(fitting_results, result_path / f'M_Fgt_cluster_errors.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Adaptive amnesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.Bayesian_new.problems.forget as forget\n",
    "importlib.reload(forget)\n",
    "from src.Bayesian_new.problems.forget import AdaptiveAmnesiaModel\n",
    "\n",
    "import src.Bayesian_new.problems.config as config\n",
    "importlib.reload(config)\n",
    "from src.Bayesian_new.problems.config import config_fgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载拟合结果\n",
    "result_path = Path(project_root) / 'results' / 'Bayesian_new'\n",
    "forget_fitting_results = joblib.load(result_path / 'M_Fgt_fitting_results_400.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_task(iSub, subject_data, subject_forget_fitting_results, alpha_gamma, alpha_w0, config):\n",
    "    condition = subject_data['condition'].iloc[0]\n",
    "    base_gamma = subject_forget_fitting_results['best_params'][0]\n",
    "    base_w0 = subject_forget_fitting_results['best_params'][1]\n",
    "    model_amn = AdaptiveAmnesiaModel(config, condition=condition, \n",
    "                                    base_gamma=base_gamma, base_w0=base_w0)\n",
    "\n",
    "    # 2) 整理数据\n",
    "    s_data = (\n",
    "        subject_data[[\"feature1\", \"feature2\", \"feature3\", \"feature4\"]].values,\n",
    "        subject_data[\"choice\"].values,\n",
    "        subject_data[\"feedback\"].values, \n",
    "        subject_data[\"category\"].values\n",
    "    )\n",
    "    \n",
    "    # 做单次计算（在 forget.py 里写好的方法）\n",
    "    step_results, mean_error = model_amn.compute_error_for_params(s_data, alpha_gamma, alpha_w0)\n",
    "    \n",
    "    return iSub, alpha_gamma, alpha_w0, mean_error, step_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_processing(learning_data, forget_fitting_results, config, alpha_gamma_values, alpha_w0_values, n_jobs=24):\n",
    "    \"\"\"\n",
    "    将 (被试, alpha_gamma, alpha_w0) 拆分为最小任务并行计算。\n",
    "    最终只保存：\n",
    "      - best_params: (alpha_gamma, alpha_w0) 最优组合\n",
    "      - best_error\n",
    "      - best_step_results (仅最优组合对应的 step 级结果)\n",
    "      - grid_errors: 一个 dict, key=(alpha_gamma,alpha_w0), value=error，用于查看各组合的误差\n",
    "    \"\"\"\n",
    "    # 1) 按被试分组\n",
    "    subjects = list(learning_data.groupby('iSub'))\n",
    "    subject_data_map = {iSub: df for iSub, df in subjects}\n",
    "\n",
    "    # 2) 枚举全部 (iSub, alpha_gamma, alpha_w0)\n",
    "    tasks = []\n",
    "    for iSub, df in subjects:\n",
    "        for alpha_gamma in alpha_gamma_values:\n",
    "            for alpha_w0 in alpha_w0_values:\n",
    "                tasks.append((iSub, alpha_gamma, alpha_w0))\n",
    "\n",
    "    # 3) 并行执行\n",
    "    results = Parallel(n_jobs=n_jobs, batch_size=1)(\n",
    "        delayed(process_single_task)(\n",
    "            iSub,\n",
    "            subject_data_map[iSub],\n",
    "            forget_fitting_results[iSub],\n",
    "            alpha_gamma,\n",
    "            alpha_w0,\n",
    "            config\n",
    "        )\n",
    "        for (iSub, alpha_gamma, alpha_w0) in tqdm(tasks, desc=\"Fitting (subject, alpha_gamma, alpha_w0)\")\n",
    "    )\n",
    "    # results 是个列表，元素形式为: (iSub, alpha_gamma, alpha_w0, mean_error, step_results)\n",
    "\n",
    "    # 4) 把结果收集到数据结构里\n",
    "    #    subject_grid_errors 用来记录对每个被试的 {(alpha_gamma, alpha_w0): error}\n",
    "    #    subject_best_combo 临时记录每个被试最优组合的 (alpha_gamma, alpha_w0, error, step_results)\n",
    "    subject_grid_errors = defaultdict(dict)\n",
    "    subject_best_combo = {}\n",
    "\n",
    "    for iSub, alpha_gamma, alpha_w0, err, step_res in results:\n",
    "        # 4.1 记录 grid_errors\n",
    "        subject_grid_errors[iSub][(alpha_gamma, alpha_w0)] = err\n",
    "\n",
    "        # 4.2 查看是否是当前最优，如果是，就更新 best\n",
    "        if iSub not in subject_best_combo:\n",
    "            subject_best_combo[iSub] = (alpha_gamma, alpha_w0, err, step_res)\n",
    "        else:\n",
    "            _, _, best_err, _ = subject_best_combo[iSub]\n",
    "            if err < best_err:\n",
    "                subject_best_combo[iSub] = (alpha_gamma, alpha_w0, err, step_res)\n",
    "\n",
    "    # 5) 最终返回的数据\n",
    "    fitting_results = {}\n",
    "    for iSub in subject_grid_errors.keys():\n",
    "        best_alpha_gamma, best_alpha_w0, best_error, best_step_results = subject_best_combo[iSub]\n",
    "        # grid_errors 这里就是简单地存 {(alpha_gamma, alpha_w0): error}\n",
    "        fitting_results[iSub] = {\n",
    "            \"condition\": subject_data_map[iSub]['condition'].iloc[0],\n",
    "            \"best_params\": (best_alpha_gamma, best_alpha_w0),\n",
    "            \"best_error\": best_error,\n",
    "            \"best_step_results\": best_step_results,\n",
    "            \"grid_errors\": subject_grid_errors[iSub]\n",
    "        }\n",
    "\n",
    "    return fitting_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 一般来说 gamma_values 和 w0_values 来自你在 forget.py 里设定的范围\n",
    "    # 比如:\n",
    "    alpha_gamma_values = np.linspace(0, 1.0, 11)\n",
    "    alpha_w0_values = np.linspace(0, 1.0, 11)\n",
    "    # w0_values = [0.15/(i+1) for i in range(20)]\n",
    "\n",
    "    # subject_data = learning_data.groupby('iSub').get_group(10)\n",
    "    fitting_results = parallel_processing(\n",
    "        learning_data=learning_data,     # 你的完整DataFrame\n",
    "        forget_fitting_results=forget_fitting_results, # 之前的拟合结果\n",
    "        config=config_fgt,              # 之前定义好的config\n",
    "        alpha_gamma_values=alpha_gamma_values,      # gamma搜索范围\n",
    "        alpha_w0_values=alpha_w0_values,            # w0搜索范围\n",
    "        n_jobs=100                       # 并行数，一般用CPU核数\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存拟合结果\n",
    "result_path = Path(project_root) / 'results' / 'Bayesian_new'\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "joblib.dump(fitting_results, result_path / 'M_Amn_fitting_results.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Unified Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 设定项目根目录\n",
    "project_root = Path(os.getcwd()).parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# 导入数据\n",
    "processed_path = Path(project_root) / 'data' / 'processed'\n",
    "learning_data = pd.read_csv(processed_path / 'Task2_processed.csv')\n",
    "\n",
    "# 导入模型\n",
    "from src.Bayesian_new import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.Bayesian_new.problems.model_new as model\n",
    "importlib.reload(model)\n",
    "from src.Bayesian_new.problems.model_new import StandardModel as Model\n",
    "\n",
    "import src.Bayesian_new.problems.config as config\n",
    "importlib.reload(config)\n",
    "from src.Bayesian_new.problems.config import config_fgt\n",
    "\n",
    "from src.Bayesian_new.problems import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_config = {\"cluster\": (PartitionCluster, {})}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.Bayesian_new.problems.model_new.StandardModel at 0x7e448789a770>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model(config, module_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_task(iSub, subject_data, gamma, w0, config, **kwargs):\n",
    "    condition = subject_data['condition'].iloc[0]\n",
    "\n",
    "    s_data = (\n",
    "        subject_data[[\"feature1\", \"feature2\", \"feature3\", \"feature4\"]].values,\n",
    "        subject_data[\"choice\"].values,\n",
    "        subject_data[\"feedback\"].values, \n",
    "        subject_data[\"category\"].values\n",
    "    )\n",
    "    module_config = {\"cluster\": (PartitionCluster, \n",
    "                                 {\"transition_spec\":[(\"random_4\", \"top_posterior\"), \n",
    "                                                     (1, \"ksimilar_centers\"), \n",
    "                                                     (2, \"random\")]})}\n",
    "    model_fgt = Model(config, module_config=module_config, condition=condition)\n",
    "\n",
    "    # 做单次计算（在 forget.py 里写好的方法）\n",
    "    step_results, mean_error = model_fgt.compute_error_for_params(s_data, gamma, w0, **kwargs)\n",
    "    \n",
    "    return iSub, gamma, w0, mean_error, step_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_processing(learning_data, config, gamma_values, w0_values, n_jobs):\n",
    "    \"\"\"\n",
    "    将 (被试, gamma, w0) 拆分为最小任务并行计算。\n",
    "    最终只保存：\n",
    "      - best_params: (gamma, w0) 最优组合\n",
    "      - best_error\n",
    "      - best_step_results (仅最优组合对应的 step 级结果)\n",
    "      - grid_errors: 一个 dict, key=(gamma,w0), value=error，用于查看各组合的误差\n",
    "    \"\"\"\n",
    "    # 1) 按被试分组\n",
    "    subjects = list(learning_data.groupby('iSub'))\n",
    "    subject_data_map = {iSub: df for iSub, df in subjects}\n",
    "\n",
    "    # 2) 枚举全部 (iSub, gamma, w0)\n",
    "    tasks = []\n",
    "    for iSub, df in subjects:\n",
    "        for gamma in gamma_values:\n",
    "            for w0 in w0_values:\n",
    "                tasks.append((iSub, gamma, w0))\n",
    "\n",
    "    # 3) 并行执行\n",
    "    \n",
    "    results = Parallel(n_jobs=n_jobs, batch_size=1)(\n",
    "        delayed(process_single_task)(\n",
    "            iSub,\n",
    "            subject_data_map[iSub],\n",
    "            gamma,\n",
    "            w0,\n",
    "            config,\n",
    "            cluster_prototype_amount=1,\n",
    "            cluster_kwargs={\"cluster_hypo_method\":\"random\",\n",
    "                            \"proto_hypo_method\":\"random\"}         \n",
    "        )\n",
    "        for (iSub, gamma, w0) in tqdm(tasks, desc=\"Fitting (subject, gamma, w0)\")\n",
    "    )\n",
    "\n",
    "    \n",
    "    # results 是个列表，元素形式为: (iSub, gamma, w0, mean_error, step_results)\n",
    "\n",
    "    # 4) 把结果收集到数据结构里\n",
    "    #    subject_grid_errors 用来记录对每个被试的 {(gamma, w0): error}\n",
    "    #    subject_best_combo 临时记录每个被试最优组合的 (gamma, w0, error, step_results)\n",
    "    subject_grid_errors = defaultdict(dict)\n",
    "    subject_best_combo = {}\n",
    "\n",
    "    for iSub, gamma, w0, err, step_res in results:\n",
    "        # 4.1 记录 grid_errors\n",
    "        subject_grid_errors[iSub][(gamma, w0)] = err\n",
    "\n",
    "        # 4.2 查看是否是当前最优，如果是，就更新 best\n",
    "        if iSub not in subject_best_combo:\n",
    "            subject_best_combo[iSub] = (gamma, w0, err, step_res)\n",
    "        else:\n",
    "            _, _, best_err, _ = subject_best_combo[iSub]\n",
    "            if err < best_err:\n",
    "                subject_best_combo[iSub] = (gamma, w0, err, step_res)\n",
    "\n",
    "    # 5) 最终返回的数据\n",
    "    fitting_results = {}\n",
    "    for iSub in subject_grid_errors.keys():\n",
    "        best_gamma, best_w0, best_error, best_step_results = subject_best_combo[iSub]\n",
    "        # grid_errors 这里就是简单地存 {(gamma, w0): error}\n",
    "        fitting_results[iSub] = {\n",
    "            \"condition\": subject_data_map[iSub]['condition'].iloc[0],\n",
    "            \"best_params\": (best_gamma, best_w0),\n",
    "            \"best_error\": best_error,\n",
    "            \"step_results\": best_step_results,\n",
    "            \"grid_errors\": subject_grid_errors[iSub]\n",
    "        }\n",
    "\n",
    "    return fitting_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting (subject, gamma, w0): 100%|██████████| 9600/9600 [06:58<00:00, 22.95it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 一般来说 gamma_values 和 w0_values 来自你在 forget.py 里设定的范围\n",
    "    # 比如:\n",
    "    gamma_values = np.linspace(0.05, 1, 20)\n",
    "    w0_values = [0.15/(i+1) for i in range(20)]\n",
    "    \n",
    "    fitting_results = parallel_processing(\n",
    "        learning_data=learning_data,     # 你的完整DataFrame\n",
    "        config=config_fgt,              # 之前定义好的config\n",
    "        gamma_values=gamma_values,      # gamma搜索范围\n",
    "        w0_values=w0_values,            # w0搜索范围\n",
    "        n_jobs=100                      # 并行数，一般用CPU核数\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/yangjiong/CategoryLearning/results/Bayesian_new/M_Fgt_c1_fitting_results.joblib']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存拟合结果\n",
    "result_path = Path(project_root) / 'results' / 'Bayesian_new'\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "joblib.dump(fitting_results, result_path / 'M_Fgt_c1_fitting_results.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
